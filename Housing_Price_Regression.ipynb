{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/umer7224/umer7224/blob/main/Housing_Price_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro Housing Price Regression Walkthrough\n",
        "In this workbook, I walk through an analysis of the Housing Price Dataset. There is also an accompanying video on YouTube located here: https://youtu.be/NQQ3DRdXAXE\n",
        "\n",
        "I touch on a few things in the **notebook**:\n",
        "1. Basic data cleaning and feature exploration\n",
        "2. Exploratory data analysis (Answering questions we have of the data)\n",
        "3. Basic Data Engineering (Creating a pipeline for tain and test sets)\n",
        "4. Model Experimentation and parameter tuning (Linear Regression, Random Forest, XGBoost, MLP)\n",
        "5. Feature Engineering\n",
        "6. Ensembling\n",
        "7. Submitting to the Competition\n",
        "\n",
        "Things I touch on in the **video**:\n",
        "1. How to approach a problem like this\n",
        "2. How I would consider using AI tools like ChatGPT to solve a problem like this\n",
        "3. Why I made certain design decisions and the choices we have we we do open ended projects like these\n",
        "4. How you can continue and improve upon this analysis\n",
        "\n",
        "I have done something similar in the past with the **Titanic Dataset** if you want something slighty more beginner friendly:\n",
        "\n",
        "- Kaggle notebook: https://www.kaggle.com/code/kenjee/titanic-project-example/notebook\n",
        "- YouTube Video: https://www.youtube.com/watch?v=I3FBJdiExcg&ab_channel=KenJee\n",
        "\n",
        "My github repos with additional free and paid resources:\n",
        "- ML Process: https://github.com/PlayingNumbers/ML_Process_Course\n",
        "- ML ALgorithms: https://github.com/PlayingNumbers/ML_Algorithms_Course"
      ],
      "metadata": {
        "id": "OV9Lxs069qYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import relevant packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "import scipy.stats as stats\n",
        "from IPython.display import display, HTML\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-29T01:00:17.521091Z",
          "iopub.execute_input": "2023-04-29T01:00:17.521562Z",
          "iopub.status.idle": "2023-04-29T01:00:17.529117Z",
          "shell.execute_reply.started": "2023-04-29T01:00:17.521523Z",
          "shell.execute_reply": "2023-04-29T01:00:17.527384Z"
        },
        "trusted": true,
        "id": "3GwODgMr9qYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Data Exploration\n",
        "1. Import the data\n",
        "2. Look at summary statisitcs\n",
        "3. Evaluate Null Values\n",
        "\n",
        "#### Want more details on EDA? Check out this notebook: https://www.kaggle.com/code/kenjee/basic-eda-example-section-6"
      ],
      "metadata": {
        "id": "lYhVW5r09qY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import data\n",
        "df = pd.read_csv(\"../input/ /train.csv\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-29T01:00:20.429133Z",
          "iopub.execute_input": "2023-04-29T01:00:20.429599Z",
          "iopub.status.idle": "2023-04-29T01:00:20.460346Z",
          "shell.execute_reply.started": "2023-04-29T01:00:20.429562Z",
          "shell.execute_reply": "2023-04-29T01:00:20.45912Z"
        },
        "trusted": true,
        "id": "1di8FWI99qY0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "99b4cb42-3f3a-4217-bb07-1f844d4433c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '../input/ /train.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-f9702c82aee2>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#import data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../input/ /train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/ /train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create scrollable table within a small window\n",
        "def create_scrollable_table(df, table_id, title):\n",
        "    html = f'<h3>{title}</h3>'\n",
        "    html += f'<div id=\"{table_id}\" style=\"height:200px; overflow:auto;\">'\n",
        "    html += df.to_html()\n",
        "    html += '</div>'\n",
        "    return html"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-29T01:00:21.729784Z",
          "iopub.execute_input": "2023-04-29T01:00:21.730241Z",
          "iopub.status.idle": "2023-04-29T01:00:21.737321Z",
          "shell.execute_reply.started": "2023-04-29T01:00:21.730198Z",
          "shell.execute_reply": "2023-04-29T01:00:21.735939Z"
        },
        "trusted": true,
        "id": "RaZLkq559qY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-29T01:02:44.564015Z",
          "iopub.execute_input": "2023-04-29T01:02:44.564508Z",
          "iopub.status.idle": "2023-04-29T01:02:44.574628Z",
          "shell.execute_reply.started": "2023-04-29T01:02:44.564455Z",
          "shell.execute_reply": "2023-04-29T01:02:44.572892Z"
        },
        "trusted": true,
        "id": "O9F3eZMI9qY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_features = df.select_dtypes(include=[np.number])\n",
        "numerical_features.describe()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-29T01:00:40.190465Z",
          "iopub.execute_input": "2023-04-29T01:00:40.190959Z",
          "iopub.status.idle": "2023-04-29T01:00:40.327834Z",
          "shell.execute_reply.started": "2023-04-29T01:00:40.19092Z",
          "shell.execute_reply": "2023-04-29T01:00:40.326417Z"
        },
        "trusted": true,
        "id": "bdS8tRdF9qY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary statistics for numerical features\n",
        "numerical_features = df.select_dtypes(include=[np.number])\n",
        "summary_stats = numerical_features.describe().T\n",
        "html_numerical = create_scrollable_table(summary_stats, 'numerical_features', 'Summary statistics for numerical features')\n",
        "\n",
        "display(HTML(html_numerical))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-29T01:00:43.240877Z",
          "iopub.execute_input": "2023-04-29T01:00:43.241364Z",
          "iopub.status.idle": "2023-04-29T01:00:43.345537Z",
          "shell.execute_reply.started": "2023-04-29T01:00:43.241322Z",
          "shell.execute_reply": "2023-04-29T01:00:43.34414Z"
        },
        "trusted": true,
        "id": "RuRZavEE9qY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary statistics for categorical features\n",
        "categorical_features = df.select_dtypes(include=[object])\n",
        "cat_summary_stats = categorical_features.describe().T\n",
        "html_categorical = create_scrollable_table(cat_summary_stats, 'categorical_features', 'Summary statistics for categorical features')\n",
        "\n",
        "display(HTML(html_categorical ))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-29T01:04:12.220508Z",
          "iopub.execute_input": "2023-04-29T01:04:12.221196Z",
          "iopub.status.idle": "2023-04-29T01:04:12.304634Z",
          "shell.execute_reply.started": "2023-04-29T01:04:12.22114Z",
          "shell.execute_reply": "2023-04-29T01:04:12.303379Z"
        },
        "trusted": true,
        "id": "JrmeY8-l9qY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Null values in the dataset\n",
        "null_values = df.isnull().sum()\n",
        "html_null_values = create_scrollable_table(null_values.to_frame(), 'null_values', 'Null values in the dataset')\n",
        "\n",
        "# Percentage of missing values for each feature\n",
        "missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
        "html_missing_percentage = create_scrollable_table(missing_percentage.to_frame(), 'missing_percentage', 'Percentage of missing values for each feature')\n",
        "\n",
        "display(HTML(html_null_values + html_missing_percentage))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-29T01:04:45.918373Z",
          "iopub.execute_input": "2023-04-29T01:04:45.919491Z",
          "iopub.status.idle": "2023-04-29T01:04:45.946852Z",
          "shell.execute_reply.started": "2023-04-29T01:04:45.919441Z",
          "shell.execute_reply": "2023-04-29T01:04:45.945594Z"
        },
        "trusted": true,
        "id": "26Co0Qlo9qY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exploring rows with missing values\n",
        "rows_with_missing_values = df[df.isnull().any(axis=1)]\n",
        "html_rows_with_missing_values = create_scrollable_table(rows_with_missing_values.head(), 'rows_with_missing_values', 'Rows with missing values')\n",
        "\n",
        "display(HTML(html_rows_with_missing_values))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-28T23:11:03.265894Z",
          "iopub.execute_input": "2023-04-28T23:11:03.267517Z",
          "iopub.status.idle": "2023-04-28T23:11:03.31017Z",
          "shell.execute_reply.started": "2023-04-28T23:11:03.267444Z",
          "shell.execute_reply": "2023-04-28T23:11:03.30853Z"
        },
        "trusted": true,
        "id": "_A9ejh4-9qY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-28T02:24:51.769188Z",
          "iopub.execute_input": "2023-04-28T02:24:51.769526Z",
          "iopub.status.idle": "2023-04-28T02:24:51.778583Z",
          "shell.execute_reply.started": "2023-04-28T02:24:51.769494Z",
          "shell.execute_reply": "2023-04-28T02:24:51.776999Z"
        },
        "trusted": true,
        "id": "3KX0g0Gv9qY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explore the dependent variable\n",
        "- Should it be normalized?\n",
        "- Normalize Dependent Vairable"
      ],
      "metadata": {
        "id": "PzNUZg9K9qY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "\n",
        "# Fit a normal distribution to the SalePrice data\n",
        "mu, sigma = stats.norm.fit(df['SalePrice'])\n",
        "\n",
        "# Create a histogram of the SalePrice column\n",
        "hist_data = go.Histogram(x=df['SalePrice'], nbinsx=50, name=\"Histogram\", opacity=0.75, histnorm='probability density', marker=dict(color='purple'))\n",
        "\n",
        "# Calculate the normal distribution based on the fitted parameters\n",
        "x_norm = np.linspace(df['SalePrice'].min(), df['SalePrice'].max(), 100)\n",
        "y_norm = stats.norm.pdf(x_norm, mu, sigma)\n",
        "\n",
        "# Create the normal distribution overlay\n",
        "norm_data = go.Scatter(x=x_norm, y=y_norm, mode=\"lines\", name=f\"Normal dist. (μ={mu:.2f}, σ={sigma:.2f})\", line=dict(color=\"green\"))\n",
        "\n",
        "# Combine the histogram and the overlay\n",
        "fig = go.Figure(data=[hist_data, norm_data])\n",
        "\n",
        "# Set the layout for the plot\n",
        "fig.update_layout(\n",
        "    title=\"SalePrice Distribution\",\n",
        "    xaxis_title=\"SalePrice\",\n",
        "    yaxis_title=\"Density\",\n",
        "    legend_title_text=\"Fitted Normal Distribution\",\n",
        "    plot_bgcolor='rgba(32, 32, 32, 1)',\n",
        "    paper_bgcolor='rgba(32, 32, 32, 1)',\n",
        "    font=dict(color='white')\n",
        ")\n",
        "\n",
        "# Create a Q-Q plot\n",
        "qq_data = stats.probplot(df['SalePrice'], dist=\"norm\")\n",
        "qq_fig = px.scatter(x=qq_data[0][0], y=qq_data[0][1], labels={'x': 'Theoretical Quantiles', 'y': 'Ordered Values'}, color_discrete_sequence=[\"purple\"])\n",
        "qq_fig.update_layout(\n",
        "    title=\"Q-Q plot\",\n",
        "    plot_bgcolor='rgba(32, 32, 32, 1)',\n",
        "    paper_bgcolor='rgba(32, 32, 32, 1)',\n",
        "    font=dict(color='white')\n",
        ")\n",
        "\n",
        "# Calculate the line of best fit\n",
        "slope, intercept, r_value, p_value, std_err = stats.linregress(qq_data[0][0], qq_data[0][1])\n",
        "line_x = np.array(qq_data[0][0])\n",
        "line_y = intercept + slope * line_x\n",
        "\n",
        "# Add the line of best fit to the Q-Q plot\n",
        "line_data = go.Scatter(x=line_x, y=line_y, mode=\"lines\", name=\"Normal Line\", line=dict(color=\"green\"))\n",
        "\n",
        "# Update the Q-Q plot with the normal line\n",
        "qq_fig.add_trace(line_data)\n",
        "\n",
        "# Show the plots\n",
        "fig.show()\n",
        "qq_fig.show()\n",
        "\n",
        "#notebook credit: https://www.kaggle.com/code/serigne/stacked-regressions-top-4-on-leaderboard"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-28T02:24:51.780236Z",
          "iopub.execute_input": "2023-04-28T02:24:51.780686Z",
          "iopub.status.idle": "2023-04-28T02:24:53.74849Z",
          "shell.execute_reply.started": "2023-04-28T02:24:51.78065Z",
          "shell.execute_reply": "2023-04-28T02:24:53.747138Z"
        },
        "trusted": true,
        "id": "jTCy2Lqb9qY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What questions do we want to ask of the data?\n",
        "\n",
        "1. Distribution of dwelling types and their relation to sale prices?\n",
        "2. Does zoning impact sale price?\n",
        "3. Does street and alley access types effect on sale price?\n",
        "4. What is the Average sale price by property shape?\n",
        "5. Is there a Correlation between Property Age and Sale Price\n",
        "6. Is there a Correlation between Living Area and Sale Price\n",
        "7. Does price change year to year?"
      ],
      "metadata": {
        "id": "rTsw2Sdy9qY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Distribution of dwelling types and their relation to sale prices\n",
        "dwelling_types = df['BldgType'].value_counts()\n",
        "dwelling_prices = df.groupby('BldgType')['SalePrice'].mean()\n",
        "\n",
        "# Format labels for the second graph\n",
        "formatted_dwelling_prices = ['$' + f'{value:,.2f}' for value in dwelling_prices.values]\n",
        "\n",
        "# Create bar charts\n",
        "fig1 = go.Figure(data=[go.Bar(\n",
        "    x=dwelling_types.index,\n",
        "    y=dwelling_types.values,\n",
        "    marker_color='rgb(76, 175, 80)',\n",
        "    text=dwelling_types.values,\n",
        "    textposition='outside',\n",
        "    width=0.4,\n",
        "    marker=dict(line=dict(width=2, color='rgba(0,0,0,1)'), opacity=1)\n",
        ")])\n",
        "fig1.update_layout(\n",
        "    title='Distribution of Building Types',\n",
        "    xaxis_title='Building Type',\n",
        "    yaxis_title='Count',\n",
        "    plot_bgcolor='rgba(34, 34, 34, 1)',\n",
        "    paper_bgcolor='rgba(34, 34, 34, 1)',\n",
        "    font=dict(color='white')\n",
        ")\n",
        "\n",
        "fig2 = go.Figure(data=[go.Bar(\n",
        "    x=dwelling_prices.index,\n",
        "    y=dwelling_prices.values,\n",
        "    marker_color='rgb(156, 39, 176)',\n",
        "    text=formatted_dwelling_prices,\n",
        "    textposition='outside',\n",
        "    width=0.4,\n",
        "    marker=dict(line=dict(width=2, color='rgba(0,0,0,1)'), opacity=1)\n",
        ")])\n",
        "fig2.update_layout(\n",
        "    title='Average Sale Price by Building Type',\n",
        "    xaxis_title='Building Type',\n",
        "    yaxis_title='Price',\n",
        "    plot_bgcolor='rgba(34, 34, 34, 1)',\n",
        "    paper_bgcolor='rgba(34, 34, 34, 1)',\n",
        "    font=dict(color='white')\n",
        ")\n",
        "\n",
        "# Show the figures\n",
        "fig1.show()\n",
        "fig2.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-28T02:24:53.754831Z",
          "iopub.execute_input": "2023-04-28T02:24:53.755337Z",
          "iopub.status.idle": "2023-04-28T02:24:53.810022Z",
          "shell.execute_reply.started": "2023-04-28T02:24:53.755293Z",
          "shell.execute_reply": "2023-04-28T02:24:53.808866Z"
        },
        "trusted": true,
        "id": "8I3f97Bm9qY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Zoning impact on sale price\n",
        "zoning_prices = df.groupby('MSZoning')['SalePrice'].mean()\n",
        "fig3 = px.bar(x=zoning_prices.index, y=zoning_prices.values, title='Average Sale Price by Zoning',\n",
        "              color_discrete_sequence=['purple', 'green'], text=zoning_prices.values,\n",
        "              template='plotly_dark')\n",
        "\n",
        "fig3.update_traces(texttemplate='$%{text:,.0f}', textposition='outside')\n",
        "fig3.update_yaxes(title='Sale Price', tickprefix='$', tickformat=',')\n",
        "fig3.update_xaxes(title='Zoning')\n",
        "fig3.update_layout(uniformtext_minsize=8, uniformtext_mode='hide')\n",
        "\n",
        "fig3.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-28T02:24:53.811785Z",
          "iopub.execute_input": "2023-04-28T02:24:53.812516Z",
          "iopub.status.idle": "2023-04-28T02:24:53.944144Z",
          "shell.execute_reply.started": "2023-04-28T02:24:53.812472Z",
          "shell.execute_reply": "2023-04-28T02:24:53.942731Z"
        },
        "trusted": true,
        "id": "Zq8H2qP89qY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Street and alley access types effect on sale price\n",
        "street_prices = df.groupby('Street')['SalePrice'].mean()\n",
        "alley_prices = df.groupby('Alley')['SalePrice'].mean()\n",
        "\n",
        "# Street Prices\n",
        "colors_street = np.where(street_prices.index == 'Pave', 'purple', 'green')\n",
        "fig5 = px.bar(x=street_prices.index, y=street_prices.values, title='Average Sale Price by Street Type',\n",
        "              template='plotly_dark', text=street_prices.values,\n",
        "              color=colors_street, color_discrete_sequence=['purple', 'green'])\n",
        "\n",
        "fig5.update_traces(texttemplate='$%{text:,.0f}', textposition='outside')\n",
        "fig5.update_yaxes(title='Sale Price', tickprefix='$', tickformat=',')\n",
        "fig5.update_xaxes(title='Street Type')\n",
        "fig5.update_layout(showlegend=False)\n",
        "\n",
        "# Alley Prices\n",
        "colors_alley = np.where(alley_prices.index == 'Pave', 'purple', 'green')\n",
        "fig6 = px.bar(x=alley_prices.index, y=alley_prices.values, title='Average Sale Price by Alley Type',\n",
        "              template='plotly_dark', text=alley_prices.values,\n",
        "              color=colors_alley, color_discrete_sequence=['purple', 'green'])\n",
        "\n",
        "fig6.update_traces(texttemplate='$%{text:,.0f}', textposition='outside')\n",
        "fig6.update_yaxes(title='Sale Price', tickprefix='$', tickformat=',')\n",
        "fig6.update_xaxes(title='Alley Type')\n",
        "fig6.update_layout(showlegend=False)\n",
        "\n",
        "fig5.show()\n",
        "fig6.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-28T02:24:53.94564Z",
          "iopub.execute_input": "2023-04-28T02:24:53.945982Z",
          "iopub.status.idle": "2023-04-28T02:24:54.099163Z",
          "shell.execute_reply.started": "2023-04-28T02:24:53.945939Z",
          "shell.execute_reply": "2023-04-28T02:24:54.097751Z"
        },
        "trusted": true,
        "id": "_BNq7mi99qY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reg: Regular\n",
        "IR1: Slightly irregular\n",
        "IR2: Moderately irregular\n",
        "IR3: Irregular"
      ],
      "metadata": {
        "id": "MSFtClie9qY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Average sale price by property shape\n",
        "colors = px.colors.qualitative.Plotly\n",
        "\n",
        "shape_prices = df.groupby('LotShape')['SalePrice'].mean()\n",
        "contour_prices = df.groupby('LandContour')['SalePrice'].mean()\n",
        "# Shape Prices\n",
        "fig7 = px.bar(x=shape_prices.index, y=shape_prices.values, title='Average Sale Price by Property Shape',\n",
        "              template='plotly_dark', text=shape_prices.values)\n",
        "\n",
        "fig7.update_traces(marker_color=colors, texttemplate='$%{text:,.0f}', textposition='outside')\n",
        "fig7.update_yaxes(title='Sale Price', tickprefix='$', tickformat=',')\n",
        "fig7.update_xaxes(title='Property Shape')\n",
        "fig7.update_layout(showlegend=False)\n",
        "\n",
        "# Contour Prices\n",
        "fig8 = px.bar(x=contour_prices.index, y=contour_prices.values, title='Average Sale Price by Property Contour',\n",
        "              template='plotly_dark', text=contour_prices.values)\n",
        "\n",
        "fig8.update_traces(marker_color=colors, texttemplate='$%{text:,.0f}', textposition='outside')\n",
        "fig8.update_yaxes(title='Sale Price', tickprefix='$', tickformat=',')\n",
        "fig8.update_xaxes(title='Property Contour')\n",
        "fig8.update_layout(showlegend=False)\n",
        "\n",
        "fig7.show()\n",
        "fig8.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-28T02:24:54.101049Z",
          "iopub.execute_input": "2023-04-28T02:24:54.101886Z",
          "iopub.status.idle": "2023-04-28T02:24:54.240216Z",
          "shell.execute_reply.started": "2023-04-28T02:24:54.101825Z",
          "shell.execute_reply": "2023-04-28T02:24:54.23882Z"
        },
        "trusted": true,
        "id": "3ZkOoaq_9qY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Calculate Property Age\n",
        "df['PropertyAge'] = df['YrSold'] - df['YearBuilt']\n",
        "\n",
        "# Calculate Correlation between Property Age and Sale Price\n",
        "age_price_corr = df['PropertyAge'].corr(df['SalePrice'])\n",
        "print(f'Correlation between Property Age and Sale Price: {age_price_corr}')\n",
        "\n",
        "# Create a scatter plot to visualize the relationship between Property Age and Sale Price\n",
        "fig9 = px.scatter(df, x='PropertyAge', y='SalePrice', title='Property Age vs Sale Price', color='PropertyAge', color_continuous_scale=px.colors.sequential.Purp)\n",
        "\n",
        "fig9.update_layout(plot_bgcolor='rgb(30,30,30)', paper_bgcolor='rgb(30,30,30)', font=dict(color='white'))\n",
        "\n",
        "fig9.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-28T02:24:54.242237Z",
          "iopub.execute_input": "2023-04-28T02:24:54.243093Z",
          "iopub.status.idle": "2023-04-28T02:24:54.342383Z",
          "shell.execute_reply.started": "2023-04-28T02:24:54.243023Z",
          "shell.execute_reply": "2023-04-28T02:24:54.341106Z"
        },
        "trusted": true,
        "id": "BBDj7cyG9qY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Calculate Correlation between Living Area and Sale Price\n",
        "living_area_price_corr = df['GrLivArea'].corr(df['SalePrice'])\n",
        "print(f'Correlation between Living Area (above grade) and Sale Price: {living_area_price_corr}')\n",
        "\n",
        "# Create a scatter plot to visualize the relationship between Living Area and Sale Price\n",
        "fig10 = px.scatter(df, x='GrLivArea', y='SalePrice', title='Living Area (above grade) vs Sale Price', color='GrLivArea', color_continuous_scale=px.colors.sequential.Purp)\n",
        "\n",
        "fig10.update_layout(plot_bgcolor='rgb(30,30,30)', paper_bgcolor='rgb(30,30,30)', font=dict(color='white'))\n",
        "\n",
        "fig10.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-28T02:24:54.344209Z",
          "iopub.execute_input": "2023-04-28T02:24:54.344686Z",
          "iopub.status.idle": "2023-04-28T02:24:54.419125Z",
          "shell.execute_reply.started": "2023-04-28T02:24:54.344636Z",
          "shell.execute_reply": "2023-04-28T02:24:54.418094Z"
        },
        "trusted": true,
        "id": "K5a0QYbx9qY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Box plot of price over the years\n",
        "yearly_avg_sale_price = df.groupby('YrSold')['SalePrice'].mean()\n",
        "\n",
        "fig13 = px.box(df, x='YrSold', y='SalePrice', title='Sale Price Trends Over the Years',\n",
        "               points=False, color_discrete_sequence=['green'])\n",
        "\n",
        "fig13.add_trace(px.line(x=yearly_avg_sale_price.index, y=yearly_avg_sale_price.values).data[0])\n",
        "\n",
        "fig13.update_traces(line=dict(color='purple', width=4), selector=dict(type='scatter', mode='lines'))\n",
        "\n",
        "for year, avg_price in yearly_avg_sale_price.items():\n",
        "    fig13.add_annotation(\n",
        "        x=year,\n",
        "        y=avg_price,\n",
        "        text=f\"{avg_price:,.0f}\",\n",
        "        font=dict(color='white'),\n",
        "        showarrow=False,\n",
        "        bgcolor='rgba(128, 0, 128, 0.6)'\n",
        "    )\n",
        "\n",
        "fig13.update_layout(\n",
        "    plot_bgcolor='rgb(30,30,30)',\n",
        "    paper_bgcolor='rgb(30,30,30)',\n",
        "    font=dict(color='white'),\n",
        "    xaxis_title='Year Sold',\n",
        "    yaxis_title='Sale Price'\n",
        ")\n",
        "\n",
        "fig13.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-28T02:24:54.420052Z",
          "iopub.execute_input": "2023-04-28T02:24:54.420433Z",
          "iopub.status.idle": "2023-04-28T02:24:54.607471Z",
          "shell.execute_reply.started": "2023-04-28T02:24:54.4204Z",
          "shell.execute_reply": "2023-04-28T02:24:54.606157Z"
        },
        "trusted": true,
        "id": "RR69U7GI9qY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a Data Pipeline\n",
        "Why do this? - So we have consistent infrastructure for transforming the test set\n",
        "\n",
        "Goal - To create infrastructure that lets us make changes without breaking everything"
      ],
      "metadata": {
        "id": "J9INTCnw9qY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "\n",
        "# Define transformers for numerical and categorical columns\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse = False))\n",
        "])\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-28T02:24:54.609295Z",
          "iopub.execute_input": "2023-04-28T02:24:54.610444Z",
          "iopub.status.idle": "2023-04-28T02:24:55.011447Z",
          "shell.execute_reply.started": "2023-04-28T02:24:54.610392Z",
          "shell.execute_reply": "2023-04-28T02:24:55.010126Z"
        },
        "trusted": true,
        "id": "Wv9EjMYX9qY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Update categorical and numerical columns\n",
        "categorical_columns = df.select_dtypes(include=['object', 'category']).columns\n",
        "numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# Remove target variable from numerical columns\n",
        "numerical_columns = numerical_columns.drop('SalePrice')\n",
        "\n",
        "# Combine transformers using ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_columns),\n",
        "        ('cat', categorical_transformer, categorical_columns)\n",
        "    ],remainder = 'passthrough')\n",
        "\n",
        "# Create a pipeline with the preprocessor\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor)])\n",
        "\n",
        "# Apply the pipeline to your dataset\n",
        "X = df.drop('SalePrice', axis=1)\n",
        "y = np.log(df['SalePrice']) #normalize dependent variable\n",
        "X_preprocessed = pipeline.fit_transform(X)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-28T02:24:55.012964Z",
          "iopub.execute_input": "2023-04-28T02:24:55.013391Z",
          "iopub.status.idle": "2023-04-28T02:24:55.082886Z",
          "shell.execute_reply.started": "2023-04-28T02:24:55.013351Z",
          "shell.execute_reply": "2023-04-28T02:24:55.081534Z"
        },
        "trusted": true,
        "id": "LB9S3L-J9qY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fit and Parameter Tune models\n",
        "- We explore some different types of models here and see how they work (or don't work)\n",
        "\n",
        "#### Want more details on Parameter Tuning? Check out this notebook: https://www.kaggle.com/code/kenjee/model-building-example-section-11\n",
        "#### Want more details on Regression Models? Check out this notebook: https://www.kaggle.com/code/kenjee/model-evaluation-regression-12"
      ],
      "metadata": {
        "id": "pfOEk9hN9qY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import GridSearchCV, KFold, cross_val_score\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the models\n",
        "models = {\n",
        "    'LinearRegression': LinearRegression(),\n",
        "    'RandomForest': RandomForestRegressor(random_state=42),\n",
        "    'XGBoost': XGBRegressor(random_state=42)\n",
        "}\n",
        "\n",
        "# Define the hyperparameter grids for each model\n",
        "param_grids = {\n",
        "    'LinearRegression': {},\n",
        "    'RandomForest': {\n",
        "        'n_estimators': [100, 200, 500],\n",
        "        'max_depth': [None, 10, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'n_estimators': [100, 200, 500],\n",
        "        'learning_rate': [0.01, 0.1, 0.3],\n",
        "        'max_depth': [3, 6, 10],\n",
        "    }\n",
        "}\n",
        "\n",
        "# 3-fold cross-validation\n",
        "cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "# Train and tune the models\n",
        "grids = {}\n",
        "for model_name, model in models.items():\n",
        "    #print(f'Training and tuning {model_name}...')\n",
        "    grids[model_name] = GridSearchCV(estimator=model, param_grid=param_grids[model_name], cv=cv, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)\n",
        "    grids[model_name].fit(X_train, y_train)\n",
        "    best_params = grids[model_name].best_params_\n",
        "    best_score = np.sqrt(-1 * grids[model_name].best_score_)\n",
        "\n",
        "    print(f'Best parameters for {model_name}: {best_params}')\n",
        "    print(f'Best RMSE for {model_name}: {best_score}\\n')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-28T02:24:55.084688Z",
          "iopub.execute_input": "2023-04-28T02:24:55.085101Z",
          "iopub.status.idle": "2023-04-28T02:29:04.046631Z",
          "shell.execute_reply.started": "2023-04-28T02:24:55.085038Z",
          "shell.execute_reply": "2023-04-28T02:29:04.045176Z"
        },
        "trusted": true,
        "id": "w2jHyR_t9qY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "X_train_scaled = X_train.copy()\n",
        "X_test_scaled = X_test.copy()\n",
        "\n",
        "# Create an MLPRegressor instance\n",
        "mlp = MLPRegressor(random_state=42,max_iter=10000, n_iter_no_change = 3,learning_rate_init=0.001)\n",
        "\n",
        "# Define the parameter grid for tuning\n",
        "param_grid = {\n",
        "    'hidden_layer_sizes': [(10,), (10,10), (10,10,10), (25)],\n",
        "    'activation': ['relu', 'tanh'],\n",
        "    'solver': ['adam'],\n",
        "    'alpha': [0.0001, 0.001, 0.01],\n",
        "    'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
        "}\n",
        "\n",
        "# Create the GridSearchCV object\n",
        "grid_search_mlp = GridSearchCV(mlp, param_grid, scoring='neg_mean_squared_error', cv=3, n_jobs=-1, verbose=1)\n",
        "\n",
        "# Fit the model on the training data\n",
        "grid_search_mlp.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Print the best parameters found during the search\n",
        "print(\"Best parameters found: \", grid_search_mlp.best_params_)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "best_score = np.sqrt(-1 * grid_search_mlp.best_score_)\n",
        "print(\"Test score: \", best_score)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-28T02:29:04.049449Z",
          "iopub.execute_input": "2023-04-28T02:29:04.050179Z",
          "iopub.status.idle": "2023-04-28T02:30:40.074186Z",
          "shell.execute_reply.started": "2023-04-28T02:29:04.050126Z",
          "shell.execute_reply": "2023-04-28T02:30:40.072762Z"
        },
        "trusted": true,
        "id": "d8UeiMKH9qY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Principal Component Analysis\n",
        "- Basic Feature Engineering"
      ],
      "metadata": {
        "id": "LA-ScCib9qY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pca\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA()\n",
        "X_pca_pre = pca.fit_transform(X_preprocessed)\n",
        "\n",
        "# Calculate the cumulative explained variance\n",
        "cumulative_explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "# Choose the number of components based on the explained variance threshold\n",
        "n_components = np.argmax(cumulative_explained_variance >= 0.95) + 1\n",
        "\n",
        "pca = PCA(n_components=n_components)\n",
        "pipeline_pca = Pipeline(steps=\n",
        "                        [('preprocessor', preprocessor),\n",
        "                        ('pca', pca)])\n",
        "\n",
        "X_pca = pipeline_pca.fit_transform(X)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-28T02:30:40.076278Z",
          "iopub.execute_input": "2023-04-28T02:30:40.07718Z",
          "iopub.status.idle": "2023-04-28T02:30:40.388952Z",
          "shell.execute_reply.started": "2023-04-28T02:30:40.077125Z",
          "shell.execute_reply": "2023-04-28T02:30:40.387076Z"
        },
        "trusted": true,
        "id": "-5z-sgqo9qY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running the same models with new data"
      ],
      "metadata": {
        "id": "vf2njuU79qY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import GridSearchCV, KFold, cross_val_score\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the models\n",
        "models = {\n",
        "    'LinearRegression': LinearRegression(),\n",
        "    'RandomForest': RandomForestRegressor(random_state=42),\n",
        "    'XGBoost': XGBRegressor(random_state=42)\n",
        "}\n",
        "\n",
        "# Define the hyperparameter grids for each model\n",
        "param_grids = {\n",
        "    'LinearRegression': {},\n",
        "    'RandomForest': {\n",
        "        'n_estimators': [100, 200, 500],\n",
        "        'max_depth': [None, 10, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'n_estimators': [100, 200, 500],\n",
        "        'learning_rate': [0.01, 0.1, 0.3],\n",
        "        'max_depth': [3, 6, 10],\n",
        "    }\n",
        "}\n",
        "\n",
        "# 3-fold cross-validation\n",
        "cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "# Train and tune the models\n",
        "grids_pca = {}\n",
        "for model_name, model in models.items():\n",
        "    #print(f'Training and tuning {model_name}...')\n",
        "    grids_pca[model_name] = GridSearchCV(estimator=model, param_grid=param_grids[model_name], cv=cv, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)\n",
        "    grids_pca[model_name].fit(X_train_pca, y_train_pca)\n",
        "    best_params = grids_pca[model_name].best_params_\n",
        "    best_score = np.sqrt(-1 * grids_pca[model_name].best_score_)\n",
        "\n",
        "    print(f'Best parameters for {model_name}: {best_params}')\n",
        "    print(f'Best RMSE for {model_name}: {best_score}\\n')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-28T02:30:40.391511Z",
          "iopub.execute_input": "2023-04-28T02:30:40.392526Z",
          "iopub.status.idle": "2023-04-28T02:36:39.665079Z",
          "shell.execute_reply.started": "2023-04-28T02:30:40.392468Z",
          "shell.execute_reply": "2023-04-28T02:36:39.663938Z"
        },
        "trusted": true,
        "id": "3Ya5iqIa9qY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "X_train_scaled_pca = X_train_pca.copy()\n",
        "X_test_scaled_pca = X_test_pca.copy()\n",
        "\n",
        "# Create an MLPRegressor instance\n",
        "mlp = MLPRegressor(random_state=42,max_iter=10000, n_iter_no_change = 3,learning_rate_init=0.001)\n",
        "\n",
        "# Define the parameter grid for tuning\n",
        "param_grid = {\n",
        "    'hidden_layer_sizes': [(10,), (10,10), (10,10,10), (25)],\n",
        "    'activation': ['relu', 'tanh'],\n",
        "    'solver': ['adam'],\n",
        "    'alpha': [0.0001, 0.001, 0.01, .1, 1],\n",
        "    'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
        "}\n",
        "\n",
        "# Create the GridSearchCV object\n",
        "grid_search_mlp_pca = GridSearchCV(mlp, param_grid, scoring='neg_mean_squared_error', cv=3, n_jobs=-1, verbose=1)\n",
        "\n",
        "# Fit the model on the training data\n",
        "grid_search_mlp_pca.fit(X_train_scaled_pca, y_train)\n",
        "\n",
        "# Print the best parameters found during the search\n",
        "print(\"Best parameters found: \", grid_search_mlp_pca.best_params_)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "best_score = np.sqrt(-1 * grid_search_mlp_pca.best_score_)\n",
        "print(\"Test score: \", best_score)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-28T02:36:39.670749Z",
          "iopub.execute_input": "2023-04-28T02:36:39.672358Z",
          "iopub.status.idle": "2023-04-28T02:42:46.291118Z",
          "shell.execute_reply.started": "2023-04-28T02:36:39.672314Z",
          "shell.execute_reply": "2023-04-28T02:42:46.289688Z"
        },
        "trusted": true,
        "id": "0Ref7T9m9qY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "for i in grids.keys():\n",
        "    print (i + ': ' + str(np.sqrt(mean_squared_error(grids[i].predict(X_test), y_test))))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-28T02:42:46.293307Z",
          "iopub.execute_input": "2023-04-28T02:42:46.29376Z",
          "iopub.status.idle": "2023-04-28T02:42:46.45881Z",
          "shell.execute_reply.started": "2023-04-28T02:42:46.293722Z",
          "shell.execute_reply": "2023-04-28T02:42:46.457856Z"
        },
        "trusted": true,
        "id": "0OWA-Xtx9qY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "for i in grids.keys():\n",
        "    print (i + ': ' + str(np.sqrt(mean_squared_error(grids_pca[i].predict(X_test_pca), y_test))))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-28T02:42:46.461992Z",
          "iopub.execute_input": "2023-04-28T02:42:46.463722Z",
          "iopub.status.idle": "2023-04-28T02:42:46.62802Z",
          "shell.execute_reply.started": "2023-04-28T02:42:46.46368Z",
          "shell.execute_reply": "2023-04-28T02:42:46.627036Z"
        },
        "trusted": true,
        "id": "6j-wKr4E9qY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print( str(np.sqrt(mean_squared_error(grid_search_mlp.predict(X_test_scaled),y_test))))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-28T02:42:46.635621Z",
          "iopub.execute_input": "2023-04-28T02:42:46.636611Z",
          "iopub.status.idle": "2023-04-28T02:42:46.658894Z",
          "shell.execute_reply.started": "2023-04-28T02:42:46.636564Z",
          "shell.execute_reply": "2023-04-28T02:42:46.656693Z"
        },
        "trusted": true,
        "id": "aZOr0cg89qY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print( str(np.sqrt(mean_squared_error(grid_search_mlp_pca.predict(X_test_scaled_pca),y_test))))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-28T02:42:46.661399Z",
          "iopub.execute_input": "2023-04-28T02:42:46.662078Z",
          "iopub.status.idle": "2023-04-28T02:42:46.67757Z",
          "shell.execute_reply.started": "2023-04-28T02:42:46.662Z",
          "shell.execute_reply": "2023-04-28T02:42:46.675839Z"
        },
        "trusted": true,
        "id": "T8KO0vAG9qY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "var_explore = df[['Fence','Alley','MiscFeature','PoolQC','FireplaceQu','GarageCond','GarageQual','GarageFinish','GarageType','BsmtExposure','BsmtFinType2','BsmtFinType1','BsmtCond','BsmtQual','MasVnrType','Electrical','MSZoning','Utilities','Exterior1st','Exterior2nd','KitchenQual','Functional','SaleType','LotFrontage','GarageYrBlt','MasVnrArea','BsmtFullBath','BsmtHalfBath','GarageCars','GarageArea','TotalBsmtSF']]\n",
        "\n",
        "display(HTML(create_scrollable_table(var_explore, 'var_explore', 'List of Variables to Explore for Feature Engineering')))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-28T02:42:46.679791Z",
          "iopub.execute_input": "2023-04-28T02:42:46.681369Z",
          "iopub.status.idle": "2023-04-28T02:42:47.452845Z",
          "shell.execute_reply.started": "2023-04-28T02:42:46.681303Z",
          "shell.execute_reply": "2023-04-28T02:42:47.451599Z"
        },
        "trusted": true,
        "id": "Ggz84Mil9qY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "# feature engineering functions\n",
        "def custom_features(df):\n",
        "    df_out = df.copy()\n",
        "    df_out['PropertyAge'] = df_out['YrSold'] - df_out['YearBuilt']\n",
        "    df_out['TotalSF'] = df_out['TotalBsmtSF'] + df_out['1stFlrSF'] + df_out['2ndFlrSF']\n",
        "    df_out['TotalBath'] = df_out['FullBath'] + 0.5 * df_out['HalfBath'] + df_out['BsmtFullBath'] + 0.5 * df['BsmtHalfBath']\n",
        "    df_out['HasRemodeled'] = (df_out['YearRemodAdd'] != df_out['YearBuilt']).astype(object)\n",
        "    df_out['Has2ndFloor'] = (df_out['2ndFlrSF'] > 0).astype(object)\n",
        "    df_out['HasGarage'] = (df_out['GarageArea'] > 0).astype(object)\n",
        "    df_out['YrSold_cat'] = df_out['YrSold'].astype(object)\n",
        "    df_out['MoSold_cat'] = df_out['MoSold'].astype(object)\n",
        "    df_out['YearBuilt_cat'] = df_out['YearBuilt'].astype(object)\n",
        "    df_out['MSSubClass_cat'] = df_out['MSSubClass'].astype(object)\n",
        "\n",
        "    return df_out\n",
        "\n",
        "feature_engineering_transformer = FunctionTransformer(custom_features)\n",
        "\n",
        "\n",
        "#make this better, one functtion? get new variables in the pipeline?"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-28T02:42:47.4543Z",
          "iopub.execute_input": "2023-04-28T02:42:47.454649Z",
          "iopub.status.idle": "2023-04-28T02:42:47.465311Z",
          "shell.execute_reply.started": "2023-04-28T02:42:47.454617Z",
          "shell.execute_reply": "2023-04-28T02:42:47.463806Z"
        },
        "trusted": true,
        "id": "RDOJzh5N9qZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify categorical and numerical columns\n",
        "new_cols_categorical = pd.Index(['HasRemodeled', 'Has2ndFloor', 'HasGarage'])\n",
        "new_cols_numeric = pd.Index(['PropertyAge', 'TotalSF', 'TotalBath', 'YrSold_cat', 'MoSold_cat', 'YearBuilt_cat', 'MSSubClass_cat'])\n",
        "\n",
        "# Update categorical and numerical columns\n",
        "categorical_columns = df.select_dtypes(include=['object', 'category']).columns.append(new_cols_categorical)\n",
        "numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns.append(new_cols_numeric)\n",
        "\n",
        "# Remove target variable from numerical columns\n",
        "numerical_columns = numerical_columns.drop('SalePrice')\n",
        "\n",
        "# Combine transformers using ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_columns),\n",
        "        ('cat', categorical_transformer, categorical_columns)\n",
        "    ],remainder = 'passthrough')\n",
        "\n",
        "# Create a pipeline with the preprocessor\n",
        "pipeline_fe = Pipeline(steps=[\n",
        "    ('fe', feature_engineering_transformer),\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('pca', pca)])\n",
        "\n",
        "# Apply the pipeline to your dataset\n",
        "X = df.drop('SalePrice', axis=1)\n",
        "y = np.log(df['SalePrice'])\n",
        "X_preprocessed_fe = pipeline_fe.fit_transform(X)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-28T02:42:47.467457Z",
          "iopub.execute_input": "2023-04-28T02:42:47.467825Z",
          "iopub.status.idle": "2023-04-28T02:42:47.633506Z",
          "shell.execute_reply.started": "2023-04-28T02:42:47.467791Z",
          "shell.execute_reply": "2023-04-28T02:42:47.631782Z"
        },
        "trusted": true,
        "id": "XjwlJVk19qZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train_fe, X_test_fe, y_train_fe, y_test_fe = train_test_split(X_preprocessed_fe, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the models\n",
        "models = {\n",
        "    'LinearRegression': LinearRegression(),\n",
        "    'RandomForest': RandomForestRegressor(random_state=42),\n",
        "    'XGBoost': XGBRegressor(random_state=42)\n",
        "}\n",
        "\n",
        "# Define the hyperparameter grids for each model\n",
        "param_grids = {\n",
        "    'LinearRegression': {},\n",
        "    'RandomForest': {\n",
        "        'n_estimators': [100, 200, 500],\n",
        "        'max_depth': [None, 10, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'n_estimators': [100, 200, 500],\n",
        "        'learning_rate': [0.01, 0.1, 0.3],\n",
        "        'max_depth': [3, 6, 10],\n",
        "    }\n",
        "}\n",
        "\n",
        "# 3-fold cross-validation\n",
        "cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "# Train and tune the models\n",
        "grids_fe = {}\n",
        "for model_name, model in models.items():\n",
        "    #print(f'Training and tuning {model_name}...')\n",
        "    grids_fe[model_name] = GridSearchCV(estimator=model, param_grid=param_grids[model_name], cv=cv, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)\n",
        "    grids_fe[model_name].fit(X_train_fe, y_train_fe)\n",
        "    best_params = grids_fe[model_name].best_params_\n",
        "    best_score = np.sqrt(-1 * grids_fe[model_name].best_score_)\n",
        "\n",
        "    print(f'Best parameters for {model_name}: {best_params}')\n",
        "    print(f'Best RMSE for {model_name}: {best_score}\\n')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-28T02:42:47.635923Z",
          "iopub.execute_input": "2023-04-28T02:42:47.637264Z",
          "iopub.status.idle": "2023-04-28T02:48:47.923956Z",
          "shell.execute_reply.started": "2023-04-28T02:42:47.637183Z",
          "shell.execute_reply": "2023-04-28T02:48:47.922831Z"
        },
        "trusted": true,
        "id": "667fzmuG9qZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_scaled_fe = X_train_fe.copy()\n",
        "X_test_scaled_fe = X_test_fe.copy()\n",
        "\n",
        "# Create an MLPRegressor instance\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "mlp = MLPRegressor(random_state=42, max_iter=10000, n_iter_no_change=3)\n",
        "\n",
        "# Define the parameter grid for tuning\n",
        "param_grid = {\n",
        "    'hidden_layer_sizes': [(10,), (10, 10), (10, 25)],\n",
        "    'activation': ['relu', 'tanh', 'sigmoid'],\n",
        "    'solver': ['adam', 'sgd'],\n",
        "    'alpha': [.1, .5, 1, 10, 100],\n",
        "    'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
        "    'learning_rate_init' : [0.1]\n",
        "}\n",
        "\n",
        "# Create the GridSearchCV object\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "grid_search_mlp_fe = GridSearchCV(mlp, param_grid, scoring='neg_mean_squared_error', cv=3, n_jobs=-1, verbose=1)\n",
        "\n",
        "# Fit the model on the training data\n",
        "grid_search_mlp_fe.fit(X_train_scaled_fe, y_train_fe)\n",
        "\n",
        "# Print the best parameters found during the search\n",
        "print(\"Best parameters found: \", grid_search_mlp_fe.best_params_)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "best_score = np.sqrt(-1 * grid_search_mlp_fe.best_score_)\n",
        "print(\"Test score: \", best_score)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-28T02:48:47.926944Z",
          "iopub.execute_input": "2023-04-28T02:48:47.928Z",
          "iopub.status.idle": "2023-04-28T02:57:03.600351Z",
          "shell.execute_reply.started": "2023-04-28T02:48:47.927958Z",
          "shell.execute_reply": "2023-04-28T02:57:03.598122Z"
        },
        "trusted": true,
        "id": "uetGycfX9qZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "for i in grids.keys():\n",
        "    print (i + ': ' + str(np.sqrt(mean_squared_error(grids_fe[i].predict(X_test_fe), y_test))))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-28T02:57:03.602754Z",
          "iopub.execute_input": "2023-04-28T02:57:03.60331Z",
          "iopub.status.idle": "2023-04-28T02:57:03.763499Z",
          "shell.execute_reply.started": "2023-04-28T02:57:03.603257Z",
          "shell.execute_reply": "2023-04-28T02:57:03.76251Z"
        },
        "trusted": true,
        "id": "VD8MTghh9qZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print( str(np.sqrt(mean_squared_error(grid_search_mlp_fe.predict(X_test_scaled_fe),y_test))))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-28T02:57:03.766481Z",
          "iopub.execute_input": "2023-04-28T02:57:03.767428Z",
          "iopub.status.idle": "2023-04-28T02:57:03.774772Z",
          "shell.execute_reply.started": "2023-04-28T02:57:03.767385Z",
          "shell.execute_reply": "2023-04-28T02:57:03.773863Z"
        },
        "trusted": true,
        "id": "yFJDxcht9qZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-28T02:57:03.775882Z",
          "iopub.execute_input": "2023-04-28T02:57:03.776508Z",
          "iopub.status.idle": "2023-04-28T02:57:03.82825Z",
          "shell.execute_reply.started": "2023-04-28T02:57:03.776472Z",
          "shell.execute_reply": "2023-04-28T02:57:03.826768Z"
        },
        "trusted": true,
        "id": "5iYJi8wI9qZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test_preprocessed = pipeline_fe.transform(df_test)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-28T02:57:03.830225Z",
          "iopub.execute_input": "2023-04-28T02:57:03.831269Z",
          "iopub.status.idle": "2023-04-28T02:57:03.909394Z",
          "shell.execute_reply.started": "2023-04-28T02:57:03.831207Z",
          "shell.execute_reply": "2023-04-28T02:57:03.907531Z"
        },
        "trusted": true,
        "id": "h13FriV49qZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#xgboost submission\n",
        "y_xgboost = np.exp(grids_fe['XGBoost'].predict(df_test_preprocessed))\n",
        "\n",
        "df_xgboost_out = df_test[['Id']].copy()\n",
        "df_xgboost_out['SalePrice'] = y_xgboost\n",
        "\n",
        "#\n",
        "df_xgboost_out.to_csv('submission_xgboost_new_features_normalized.csv', index=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-28T02:57:03.915264Z",
          "iopub.execute_input": "2023-04-28T02:57:03.918586Z",
          "iopub.status.idle": "2023-04-28T02:57:03.981542Z",
          "shell.execute_reply.started": "2023-04-28T02:57:03.918498Z",
          "shell.execute_reply": "2023-04-28T02:57:03.980448Z"
        },
        "trusted": true,
        "id": "vNUneKid9qZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rf submission\n",
        "y_rf = np.exp(grids_fe['RandomForest'].predict(df_test_preprocessed))\n",
        "\n",
        "df_rf_out = df_test[['Id']].copy()\n",
        "df_rf_out['SalePrice'] = y_rf\n",
        "\n",
        "#\n",
        "df_rf_out.to_csv('submission_rf_normalized.csv', index=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-28T02:57:03.985944Z",
          "iopub.execute_input": "2023-04-28T02:57:03.989606Z",
          "iopub.status.idle": "2023-04-28T02:57:04.198038Z",
          "shell.execute_reply.started": "2023-04-28T02:57:03.989504Z",
          "shell.execute_reply": "2023-04-28T02:57:04.196636Z"
        },
        "trusted": true,
        "id": "nwA-TxzI9qZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mlp submission\n",
        "y_mlp = np.exp(grid_search_mlp_fe.predict(df_test_preprocessed))\n",
        "\n",
        "df_mlp_out = df_test[['Id']].copy()\n",
        "df_mlp_out['SalePrice'] = y_mlp\n",
        "\n",
        "df_mlp_out.to_csv('submission_mlp_normalized.csv', index=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-28T02:57:04.199853Z",
          "iopub.execute_input": "2023-04-28T02:57:04.200366Z",
          "iopub.status.idle": "2023-04-28T02:57:04.231182Z",
          "shell.execute_reply.started": "2023-04-28T02:57:04.200317Z",
          "shell.execute_reply": "2023-04-28T02:57:04.229142Z"
        },
        "trusted": true,
        "id": "GyeN-YrH9qZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_avg_ens = (y_rf + y_xgboost + y_mlp)/3\n",
        "\n",
        "#xgboost submission\n",
        "df_avg_ens_out = df_test[['Id']].copy()\n",
        "df_avg_ens_out['SalePrice'] = y_avg_ens\n",
        "\n",
        "#\n",
        "df_avg_ens_out.to_csv('submission_avg_ens_new_features_normalized.csv', index=False)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-28T02:57:04.233717Z",
          "iopub.execute_input": "2023-04-28T02:57:04.234427Z",
          "iopub.status.idle": "2023-04-28T02:57:04.258522Z",
          "shell.execute_reply.started": "2023-04-28T02:57:04.234347Z",
          "shell.execute_reply": "2023-04-28T02:57:04.256371Z"
        },
        "trusted": true,
        "id": "Kmp9ArwL9qZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import StackingRegressor\n",
        "\n",
        "grids_fe['MLP'] =   grid_search_mlp_fe\n",
        "\n",
        "best_estimators = [(model_name, grid.best_estimator_) for model_name, grid in grids_fe.items()]\n",
        "\n",
        "# Define the candidate meta-models\n",
        "meta_models = {\n",
        "    'MLP': MLPRegressor(random_state=42, max_iter=10000, n_iter_no_change=3, learning_rate_init=0.001),\n",
        "    'LinearRegression': LinearRegression(),\n",
        "    'XGBoost': XGBRegressor(random_state=42)\n",
        "}\n",
        "\n",
        "# Define the hyperparameter grids for each meta-model\n",
        "meta_param_grids = {\n",
        "    'MLP': {\n",
        "        'final_estimator__hidden_layer_sizes': [(10,), (10, 10)],\n",
        "        'final_estimator__activation': ['relu', 'tanh'],\n",
        "        'final_estimator__solver': ['adam', 'sgd'],\n",
        "        'final_estimator__alpha': [ 0.001, 0.01, .1, .5],\n",
        "        'final_estimator__learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
        "    },\n",
        "    'LinearRegression': {},\n",
        "    'XGBoost': {\n",
        "        'final_estimator__n_estimators': [100, 200, 500],\n",
        "        'final_estimator__learning_rate': [0.01, 0.1, 0.3],\n",
        "        'final_estimator__max_depth': [3, 6, 10],\n",
        "    }\n",
        "}\n",
        "\n",
        "# 3-fold cross-validation\n",
        "cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "# Train and tune the stacking ensemble\n",
        "best_score = float('inf')\n",
        "best_model = None\n",
        "\n",
        "for meta_name, meta_model in meta_models.items():\n",
        "    print(f'Training and tuning {meta_name} as the meta-model...')\n",
        "    stacking_regressor = StackingRegressor(estimators=best_estimators, final_estimator=meta_model, cv=cv)\n",
        "    grid_search = GridSearchCV(estimator=stacking_regressor, param_grid=meta_param_grids[meta_name], cv=cv, scoring='neg_mean_squared_error', n_jobs=-1, verbose=1)\n",
        "    grid_search.fit(X_train_fe, y_train_fe)\n",
        "    best_params = grid_search.best_params_\n",
        "    best_rmse = np.sqrt(-1 * grid_search.best_score_)\n",
        "\n",
        "    print(f'Best parameters for {meta_name}: {best_params}')\n",
        "    print(f'Best RMSE for {meta_name}: {best_rmse}\\n')\n",
        "\n",
        "    if best_rmse < best_score:\n",
        "        best_score = best_rmse\n",
        "        best_model = grid_search\n",
        "\n",
        "# Evaluate the best stacking ensemble on the test data\n",
        "y_pred = best_model.predict(X_test_fe)\n",
        "rmse = np.sqrt(mean_squared_error(y_test_fe, y_pred))\n",
        "print(f\"Best stacking ensemble's RMSE on test data: {rmse}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-28T02:57:04.261117Z",
          "iopub.execute_input": "2023-04-28T02:57:04.262299Z",
          "iopub.status.idle": "2023-04-28T05:40:21.018687Z",
          "shell.execute_reply.started": "2023-04-28T02:57:04.262244Z",
          "shell.execute_reply": "2023-04-28T05:40:21.017635Z"
        },
        "trusted": true,
        "id": "ApOjQldA9qZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_stack = np.exp(best_model.predict(df_test_preprocessed))\n",
        "\n",
        "#xgboost submission\n",
        "\n",
        "df_stack_out = df_test[['Id']].copy()\n",
        "df_stack_out['SalePrice'] = y_stack\n",
        "\n",
        "df_stack_out.to_csv('submission_stack_new_features_normalized.csv', index=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-28T06:53:48.248832Z",
          "iopub.execute_input": "2023-04-28T06:53:48.249299Z",
          "iopub.status.idle": "2023-04-28T06:53:48.545674Z",
          "shell.execute_reply.started": "2023-04-28T06:53:48.249257Z",
          "shell.execute_reply": "2023-04-28T06:53:48.544561Z"
        },
        "trusted": true,
        "id": "AKkAObkF9qZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_stack_out.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-28T06:55:53.302472Z",
          "iopub.execute_input": "2023-04-28T06:55:53.30289Z",
          "iopub.status.idle": "2023-04-28T06:55:53.314627Z",
          "shell.execute_reply.started": "2023-04-28T06:55:53.302856Z",
          "shell.execute_reply": "2023-04-28T06:55:53.313262Z"
        },
        "trusted": true,
        "id": "bgEFQio39qZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check out these additional Resources for continued learning!\n",
        "\n",
        "### Titanic Beginner Walkthrough\n",
        "- Kaggle notebook: https://www.kaggle.com/code/kenjee/titanic-project-example/notebook\n",
        "- YouTube Video: https://www.youtube.com/watch?v=I3FBJdiExcg&ab_channel=KenJee\n",
        "\n",
        "### My Other Notebooks for Learning the ML Process\n",
        "- [**Dealing with Missing Values - Section 5.1**](https://www.kaggle.com/code/kenjee/dealing-with-missing-values-section-5-1)\n",
        "- [**Dealing with Outliers - Section 5.2**](https://www.kaggle.com/code/kenjee/dealing-with-outliers-section-5-2)\n",
        "- [**Basic EDA Example - Section 6**](https://www.kaggle.com/code/kenjee/basic-eda-example-section-6)\n",
        "- [**Categorical Feature Engineering - Section 7.1**](https://www.kaggle.com/code/kenjee/categorical-feature-engineering-section-7-1)\n",
        "- [**Numeric Feature Engineering - Section 7.2**](https://www.kaggle.com/kenjee/numeric-feature-engineering-section-7-2)\n",
        "- [**Cross Validation Foundations - Section 8**](https://www.kaggle.com/code/kenjee/cross-validation-foundations-section-8)\n",
        "- [**Feature Selection - Section 9**](https://www.kaggle.com/code/kenjee/feature-selection-section-9)\n",
        "- [**Dealing with Imbalanced Data - Section 10**](https://www.kaggle.com/code/kenjee/dealing-with-imbalanced-data-section-10)\n",
        "- [**Model Building Example - Section 11**](https://www.kaggle.com/code/kenjee/model-building-example-section-11)\n",
        "- [**Model Evaluation (Classification) - Section 11**](https://www.kaggle.com/code/kenjee/model-evaluation-classification-section-12)\n",
        "- [**Model Evlauation (Regression) - Section 11**](https://www.kaggle.com/code/kenjee/model-evaluation-regression-12)\n",
        "\n",
        "### My Other Notebooks for Learning ML Algorithms\n",
        "- [**Exhaustive Regression with Parameter Tuning**](https://www.kaggle.com/code/kenjee/exhaustive-regression-parameter-tuning)\n",
        "- [**Exhaustive Classification with Parameter Tuning**](https://www.kaggle.com/code/kenjee/exhaustive-classification-parameter-tuning)\n"
      ],
      "metadata": {
        "id": "_iw6pKDR9qZI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "heyXA8N99qZI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}